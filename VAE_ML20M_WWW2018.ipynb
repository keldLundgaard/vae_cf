{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoders for collaborative filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the paper \"*Variational autoencoders for collaborative filtering*\" by Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara, in The Web Conference (aka WWW) 2018.\n",
    "\n",
    "In this notebook, we will show a complete self-contained example of training a variational autoencoder (as well as a denoising autoencoder) with multinomial likelihood (described in the paper) on the public Movielens-20M dataset, including both data preprocessing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import apply_regularization, l2_regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "# \n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and create train/validation/test splits following strong generalization: \n",
    "\n",
    "- We split all users into training/validation/test sets. \n",
    "\n",
    "- We train models using the entire click history of the training users. \n",
    "\n",
    "- To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dataset at http://files.grouplens.org/datasets/movielens/ml-20m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### change `DATA_DIR` to the location where movielens-20m dataset sits\n",
    "DATA_DIR = '/data/projects/vae_cf/ml-20m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "raw_data = raw_data[raw_data['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1094785734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112485573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating   timestamp\n",
       "6        1      151     4.0  1094785734\n",
       "7        1      223     4.0  1112485573\n",
       "8        1      253     4.0  1112484940\n",
       "9        1      260     4.0  1112484826\n",
       "10       1      293     4.0  1112484703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
    "- Use all the items from the training users as item set\n",
    "- For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep items that are clicked on by at least 5 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 9990682 watching events from 136677 users and 20720 movies (sparsity: 0.353%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: profile2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two related models: denoising autoencoder with multinomial likelihood (Multi-DAE in the paper) and partially-regularized variational autoencoder with multinomial likelihood (Multi-VAE^{PR} in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: We use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. In this work, we consider learning with implicit feedback. The user-by-item interaction matrix is the click matrix $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$. The lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ is a bag-of-words vector with the number of clicks for each item from user u. We binarize the click matrix. It is straightforward to extend it to general count data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generative process__: For each user $u$, the model starts by sampling a $K$-dimensional latent representation $\\mathbf{z}_u$ from a standard Gaussian prior. The latent representation $\\mathbf{z}_u$ is transformed via a non-linear function $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ to produce a probability distribution over $I$ items $\\pi (\\mathbf{z}_u)$ from which the click history $\\mathbf{x}_u$ is assumed to have been drawn:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_u \\sim \\mathcal{N}(0, \\mathbf{I}_K),  \\pi(\\mathbf{z}_u) \\propto \\exp\\{f_\\theta (\\mathbf{z}_u\\},\\\\\n",
    "\\mathbf{x}_u \\sim \\mathrm{Mult}(N_u, \\pi(\\mathbf{z}_u))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for Multi-DAE for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\log p_\\theta(\\mathbf{x}_u | g_\\phi(\\mathbf{x}_u))\n",
    "$$\n",
    "where $g_\\phi(\\cdot)$ is the non-linear \"encoder\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiDAE(object):\n",
    "    def __init__(self, p_dims, q_dims=None, lam=0.01, lr=1e-3, random_seed=None):\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims is None:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "        else:\n",
    "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "        \n",
    "        self.lam = lam\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.construct_placeholders()\n",
    "\n",
    "    def construct_placeholders(self):        \n",
    "        self.input_ph = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.dims[0]])\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self.construct_weights()\n",
    "\n",
    "        saver, logits = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        # per-user average negative log-likelihood\n",
    "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
    "            log_softmax_var * self.input_ph, axis=1))\n",
    "        # apply regularization to weights\n",
    "        reg = l2_regularizer(self.lam)\n",
    "        reg_var = apply_regularization(reg, self.weights)\n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        loss = neg_ll + 2 * reg_var\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return saver, logits, loss, train_op, merged\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # construct forward graph        \n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return tf.train.Saver(), h\n",
    "\n",
    "    def construct_weights(self):\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # define weights\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dims[:-1], self.dims[1:])):\n",
    "            weight_key = \"weight_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_{}\".format(i+1)\n",
    "            \n",
    "            self.weights.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of Multi-VAE^{PR} (evidence lower-bound, or ELBO) for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n",
    "$$\n",
    "where $q_\\phi$ is the approximating variational distribution (inference model). $\\beta$ is the additional annealing parameter that we control. The objective of the entire dataset is the average over all the users. It can be trained almost the same as Multi-DAE, thanks to reparametrization trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiVAE(MultiDAE):\n",
    "\n",
    "    def construct_placeholders(self):\n",
    "        super(MultiVAE, self).construct_placeholders()\n",
    "\n",
    "        # placeholders with default values when scoring\n",
    "        self.is_training_ph = tf.placeholder_with_default(0., shape=None)\n",
    "        self.anneal_ph = tf.placeholder_with_default(1., shape=None)\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self._construct_weights()\n",
    "\n",
    "        saver, logits, KL = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
    "            log_softmax_var * self.input_ph,\n",
    "            axis=-1))\n",
    "        # apply regularization to weights\n",
    "        reg = l2_regularizer(self.lam)\n",
    "        \n",
    "        reg_var = apply_regularization(reg, self.weights_q + self.weights_p)\n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        neg_ELBO = neg_ll + self.anneal_ph * KL + 2 * reg_var\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(self.lr).minimize(neg_ELBO)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.summary.scalar('KL', KL)\n",
    "        tf.summary.scalar('neg_ELBO_train', neg_ELBO)\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        return saver, logits, neg_ELBO, train_op, merged\n",
    "    \n",
    "    def q_graph(self):\n",
    "        mu_q, std_q, KL = None, None, None\n",
    "        \n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights_q) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "            else:\n",
    "                mu_q = h[:, :self.q_dims[-1]]\n",
    "                logvar_q = h[:, self.q_dims[-1]:]\n",
    "\n",
    "                std_q = tf.exp(0.5 * logvar_q)\n",
    "                KL = tf.reduce_mean(tf.reduce_sum(\n",
    "                        0.5 * (-logvar_q + tf.exp(logvar_q) + mu_q**2 - 1), axis=1))\n",
    "        return mu_q, std_q, KL\n",
    "\n",
    "    def p_graph(self, z):\n",
    "        h = z\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights_p) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # q-network\n",
    "        mu_q, std_q, KL = self.q_graph()\n",
    "        epsilon = tf.random_normal(tf.shape(std_q))\n",
    "\n",
    "        sampled_z = mu_q + self.is_training_ph *\\\n",
    "            epsilon * std_q\n",
    "\n",
    "        # p-network\n",
    "        logits = self.p_graph(sampled_z)\n",
    "        \n",
    "        return tf.train.Saver(), logits, KL\n",
    "\n",
    "    def _construct_weights(self):\n",
    "        self.weights_q, self.biases_q = [], []\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
    "            if i == len(self.q_dims[:-1]) - 1:\n",
    "                # we need two sets of parameters for mean and variance,\n",
    "                # respectively\n",
    "                d_out *= 2\n",
    "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_q_{}\".format(i+1)\n",
    "            \n",
    "            self.weights_q.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_q.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights_q[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases_q[-1])\n",
    "            \n",
    "        self.weights_p, self.biases_p = [], []\n",
    "\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
    "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_p_{}\".format(i+1)\n",
    "            self.weights_p.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_p.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights_p[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases_p[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/validation data, hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-processed training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n",
    "                                           os.path.join(pro_dir, 'validation_te.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = train_data.shape[0]\n",
    "idxlist = range(N)\n",
    "\n",
    "# training batch size\n",
    "batch_size = 200\n",
    "batches_per_epoch = int(np.ceil(float(N) / batch_size))\n",
    "\n",
    "N_vad = vad_data_tr.shape[0]\n",
    "idxlist_vad = range(N_vad)\n",
    "\n",
    "# validation batch size (since the entire validation set might not fit into GPU memory)\n",
    "batch_size_vad = 1000\n",
    "\n",
    "# the total number of gradient updates for annealing\n",
    "total_anneal_steps = 200000\n",
    "# largest annealing parameter\n",
    "anneal_cap = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Multi-VAE^{PR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML-20M dataset, we set both the generative function $f_\\theta(\\cdot)$ and the inference model $g_\\phi(\\cdot)$ to be 3-layer multilayer perceptron (MLP) with symmetrical architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function is a [200 -> 600 -> n_items] MLP, which means the inference function is a [n_items -> 600 -> 200] MLP. Thus the overall architecture for the Multi-VAE^{PR} is [n_items -> 600 -> 200 -> 600 -> n_items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_dims = [200, 600, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = vae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging and checkpoint directory\n",
    "\n",
    "- Change all the logging directory and checkpoint directory to somewhere of your choice\n",
    "- Monitor training progress using tensorflow by: `tensorboard --logdir=$log_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in vae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: /home/keld/projects/vae_cf/log/ml-20m/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/home/keld/projects/vae_cf/log/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /home/keld/projects/vae_cf/log/ml-20m/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/home/keld/projects/vae_cf/log/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir) \n",
    "    \n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxlist = np.array(idxlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [2:45:22<00:00, 49.67s/it]\n"
     ]
    }
   ],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "\n",
    "    update_count = 0.0\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs), total=n_epochs):\n",
    "        np.random.shuffle(idxlist)\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "            \n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')           \n",
    "            \n",
    "            if total_anneal_steps > 0:\n",
    "                anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "            else:\n",
    "                anneal = anneal_cap\n",
    "            \n",
    "            feed_dict = {vae.input_ph: X, \n",
    "                         vae.keep_prob_ph: 0.5, \n",
    "                         vae.anneal_ph: anneal,\n",
    "                         vae.is_training_ph: 1}        \n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, \n",
    "                                           global_step=epoch * batches_per_epoch + bnum) \n",
    "            \n",
    "            update_count += 1\n",
    "        \n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "        \n",
    "            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "        \n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe6c153c208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test data and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_tr, test_data_te = load_tr_te_data(\n",
    "    os.path.join(pro_dir, 'test_tr.csv'),\n",
    "    os.path.join(pro_dir, 'test_te.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_test = test_data_tr.shape[0]\n",
    "idxlist_test = range(N_test)\n",
    "\n",
    "batch_size_test = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0)\n",
    "saver, logits_var, _, _, _ = vae.build_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /home/keld/projects/vae_cf/log/ml-20m/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/home/keld/projects/vae_cf/log/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/keld/projects/vae_cf/log/ml-20m/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I/model\n"
     ]
    }
   ],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "    \n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NDCG@100=0.42194 (0.00210)\n",
      "Test Recall@20=0.39042 (0.00270)\n",
      "Test Recall@50=0.53218 (0.00286)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Multi-DAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function is a [200 -> n_items] MLP, thus the overall architecture for the Multi-DAE is [n_items -> 200 -> n_items]. We find this architecture achieves better validation NDCG@100 than the [n_items -> 600 -> 200 -> 600 -> n_items] architecture as used in Multi-VAE^{PR}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_dims = [200, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = dae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging and checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in dae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: /home/keld/projects/vae_cf/log/ml-20m/DAE/I-200-I\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/home/keld/projects/vae_cf/log/ml-20m/DAE/{}'.format(arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /home/keld/projects/vae_cf/log/ml-20m/DAE/I-200-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/home/keld/projects/vae_cf/log/ml-20m/DAE/{}'.format(arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir) \n",
    "    \n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [15:51<2:02:15, 41.44s/it]"
     ]
    }
   ],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        np.random.shuffle(idxlist)\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "            \n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')           \n",
    "            \n",
    "            feed_dict = {dae.input_ph: X, \n",
    "                         dae.keep_prob_ph: 0.5}        \n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, global_step=epoch * batches_per_epoch + bnum) \n",
    "                    \n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "        \n",
    "            pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "        \n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size)\n",
    "saver, logits_var, _, _, _ = dae.build_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_dir = '/home/keld/projects/vae_cf/chkpt/ml-20m/DAE/{}'.format(arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "    \n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "\n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
